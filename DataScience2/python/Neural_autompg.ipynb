{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c857c22a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      3\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGD, Adam\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "#import tensorflow\n",
    "import warnings\n",
    "warnings.filterwarnings ('ignore')\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "def calculate_adj_r2(r_sq, k, n):\n",
    "\n",
    "    adj_r = 1-((1-r_sq)*(n-1)/(n-k-1))\n",
    "   \n",
    "    return adj_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e114409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LabelEncode(df_local):\n",
    "    for col in df_local.columns:\n",
    "        col_dict = {}\n",
    "        if df_local[col].dtype!='object':\n",
    "            continue\n",
    "        elif df_local[col].dtype=='object':\n",
    "            col2=df_local[col].unique()\n",
    "            z=0\n",
    "            for i in col2:\n",
    "                col_dict[i]=z\n",
    "                z+=1\n",
    "            df_local[col]=df_local[col].map(col_dict)\n",
    "        \n",
    "            \n",
    "    return df_local\n",
    "\n",
    "data=pd.read_csv(\"auto-mpg.csv\")\n",
    "data.head()\n",
    "\n",
    "print(data.columns)\n",
    "data.fillna(0)\n",
    "print(data.columns)\n",
    "df_encode = LabelEncode(data)\n",
    "df_encode.shape\n",
    "\n",
    "X = ['cylinders', 'displacement', 'horsepower', 'weight',\n",
    "        'model year', 'origin', 'car name' ]\n",
    "#X=['weight', 'model year', 'origin']\n",
    "\n",
    "Y = ['mpg']\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(df_encode[X],df_encode[Y], test_size=0.25, random_state=42, shuffle=True)\n",
    "# df_train=pd.concat([X_train, Y_train], axis=1).reindex(X_train.index)\n",
    "# df_test=pd.concat([X_test, Y_test], axis=1).reindex(X_test.index)\n",
    "# print(df_train.size)\n",
    "# print(df_test.size)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_encode[X],df_encode[Y], test_size=0.25, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c5eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale both training and testing input data\n",
    "\n",
    "X_train = preprocessing.scale(X_train)\n",
    "\n",
    "X_test = preprocessing.scale(X_test)\n",
    "\n",
    "activation_list = ['relu', 'sigmoid', 'tanh', 'elu']\n",
    "r2_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b8958",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines linear regression model and its structure\n",
    "for i in activation_list:\n",
    "    print(\"Activation Function:\" , i)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1,input_dim=len(X), activation=i))\n",
    "    model.compile(optimizer='adam', loss='mae', metrics=['mae']) \n",
    "    #model.compile(Adam(lr=0.003), 'mean_squared_error')\n",
    "\n",
    "\n",
    "    #Fits model\n",
    "    history = model.fit(X_train, Y_train, epochs = 100, validation_split = 0.1,verbose = 0)\n",
    "    history_dict=history.history\n",
    "    print(history_dict.keys())\n",
    "    #Plots model's training cost/loss and model's validation split cost/loss\n",
    "    loss_values = history_dict['loss']\n",
    "    val_loss_values=history_dict['val_loss']\n",
    "    #plt.figure()\n",
    "    plt.plot(loss_values,label='training loss')\n",
    "    plt.plot(val_loss_values,label='val training loss')\n",
    "    plt.show()\n",
    "    # Runs model (the one with the activation function, although this doesn't really matter as they perform the same) \n",
    "    # with its current weights on the training and testing data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    epochs = range(1, len(loss_values) + 1)\n",
    "    plt.subplots(1,2,figsize=(10,5))\n",
    "    plt.plot(epochs, loss_values, '-', label='Training MAE')\n",
    "    plt.plot(epochs, val_loss_values, ':', label='Validation MAE')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.plot()\n",
    "\n",
    "    # Calculates and prints r2 score of training and testing data\n",
    "    #print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(Y_train, y_train_pred)))\n",
    "    r2_list.append(r2_score(Y_test, y_test_pred))\n",
    "    #print(\"The R2 score on the Test set is:\\t{:0.3f}\".format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92830429",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Neural Net 2 Layers\")\n",
    "\n",
    "for i in range(len(activation_list)):\n",
    "    print(\"R2 value for using activation function %s : %s\", activation_list[i], r2_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines linear regression model and its structure\n",
    "r2_list = []\n",
    "for i in activation_list:\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(1, input_shape=(7,)))\n",
    "    model.add(Dense(13, input_dim=len(X), activation=i))\n",
    "    model.add(Dense(13))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    #Compiles model\n",
    "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    # optimizer=’adam’ tells Keras to use the Adam optimizer to adjust weights and biases in each \n",
    "    # backpropagation pass during training. Adam is one of eight optimizers that are built into Keras,\n",
    "    # and it is among the most advanced. \n",
    "    # It uses an adaptive learning rate and it is always the one I start with in the absence of a compelling reason to do otherwise.\n",
    "    model.compile(optimizer='adam', loss='mae', metrics=['mae']) \n",
    "    #model.compile(Adam(lr=0.003), 'mean_squared_error')\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    #Fits model\n",
    "    history = model.fit(X_train, Y_train, epochs = 100, validation_split = 0.1,verbose = 0)\n",
    "    history_dict=history.history\n",
    "    print(history_dict.keys())\n",
    "    #Plots model's training cost/loss and model's validation split cost/loss\n",
    "    loss_values = history_dict['loss']\n",
    "    val_loss_values=history_dict['val_loss']\n",
    "    #plt.figure()\n",
    "    plt.plot(loss_values,label='training loss')\n",
    "    plt.show()\n",
    "    # Runs model (the one with the activation function, although this doesn't really matter as they perform the same) \n",
    "    # with its current weights on the training and testing data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "    plt.plot(epochs, loss_values, '-', label='Training MAE')\n",
    "    plt.plot(epochs, val_loss_values, ':', label='Validation MAE')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.plot()\n",
    "    r2_list.append(r2_score(Y_test, y_test_pred))\n",
    "    # Calculates and prints r2 score of training and testing data\n",
    "#     print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(Y_train, y_train_pred)))\n",
    "#     print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(Y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f743e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Neural Net 3 Layers\")\n",
    "for i in range(len(activation_list)):\n",
    "    print(\"R2 value for using activation function %s : %s\", activation_list[i], r2_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafc2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_list = []\n",
    "for i in activation_list:\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(1, input_shape=(7,)))\n",
    "    model.add(Dense(13, input_dim=len(X), activation=i))\n",
    "    model.add(Dense(13))\n",
    "    model.add(Dense(13))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    #Compiles model\n",
    "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    # optimizer=’adam’ tells Keras to use the Adam optimizer to adjust weights and biases in each \n",
    "    # backpropagation pass during training. Adam is one of eight optimizers that are built into Keras,\n",
    "    # and it is among the most advanced. \n",
    "    # It uses an adaptive learning rate and it is always the one I start with in the absence of a compelling reason to do otherwise.\n",
    "    model.compile(optimizer='adam', loss='mae', metrics=['mae']) \n",
    "    #model.compile(Adam(lr=0.003), 'mean_squared_error')\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    #Fits model\n",
    "    history = model.fit(X_train, Y_train, epochs = 100, validation_split = 0.1,verbose = 0)\n",
    "    history_dict=history.history\n",
    "    print(history_dict.keys())\n",
    "    #Plots model's training cost/loss and model's validation split cost/loss\n",
    "    loss_values = history_dict['loss']\n",
    "    val_loss_values=history_dict['val_loss']\n",
    "    #plt.figure()\n",
    "    plt.plot(loss_values,label='training loss')\n",
    "    plt.show()\n",
    "    # Runs model (the one with the activation function, although this doesn't really matter as they perform the same) \n",
    "    # with its current weights on the training and testing data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "    plt.plot(epochs, loss_values, '-', label='Training MAE')\n",
    "    plt.plot(epochs, val_loss_values, ':', label='Validation MAE')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.plot()\n",
    "    r2_list.append(r2_score(Y_test, y_test_pred))\n",
    "    # Calculates and prints r2 score of training and testing data\n",
    "#     print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(Y_train, y_train_pred)))\n",
    "#     print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(Y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02990310",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Neural Net 4 Layers\")\n",
    "for i in range(len(activation_list)):\n",
    "    print(\"R2 value for using activation function %s : %s\", activation_list[i], r2_list[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
